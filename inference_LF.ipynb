{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating images with MX-Font model from a reference style\n",
    "In this example we'll generate images with trained LF-Font model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Loading packages\n",
    "* First, load the packages used in this code.\n",
    "* All of the packages are avilable in `pip`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from itertools import chain\n",
    "\n",
    "import torch\n",
    "from sconf import Config\n",
    "from torchvision import transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* These modules are defined in this repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from base.dataset import read_font, render\n",
    "from base.utils import save_tensor_to_image, load_reference\n",
    "from LF.models import Generator\n",
    "from inference import infer_LF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Build model\n",
    "* Build and load the trained model.\n",
    "* `weight_path` : \n",
    "    * The location of the trained model weight.\n",
    "* `emb_dim` :\n",
    "    * The dimension of embedding blocks in the trained model weight.\n",
    "* `decomposition` :\n",
    "    * The location of the pre-defined decomposition rule file.\n",
    "* `primals` :\n",
    "    * The location of the primals list file.\n",
    "    * The order of primals list should be identical to that used for the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Generator:\n\tsize mismatch for style_emb_blocks.last.w: copying a param with shape torch.Size([8, 256, 1, 1]) from checkpoint, the shape in current model is torch.Size([4, 256, 1, 1]).\n\tsize mismatch for style_emb_blocks.last.b: copying a param with shape torch.Size([8]) from checkpoint, the shape in current model is torch.Size([4]).\n\tsize mismatch for style_emb_blocks.skip.w: copying a param with shape torch.Size([8, 128, 1, 1]) from checkpoint, the shape in current model is torch.Size([4, 128, 1, 1]).\n\tsize mismatch for style_emb_blocks.skip.b: copying a param with shape torch.Size([8]) from checkpoint, the shape in current model is torch.Size([4]).\n\tsize mismatch for comp_emb_blocks.last.w: copying a param with shape torch.Size([8, 256, 1, 1]) from checkpoint, the shape in current model is torch.Size([4, 256, 1, 1]).\n\tsize mismatch for comp_emb_blocks.last.b: copying a param with shape torch.Size([8]) from checkpoint, the shape in current model is torch.Size([4]).\n\tsize mismatch for comp_emb_blocks.skip.w: copying a param with shape torch.Size([8, 128, 1, 1]) from checkpoint, the shape in current model is torch.Size([4, 128, 1, 1]).\n\tsize mismatch for comp_emb_blocks.skip.b: copying a param with shape torch.Size([8]) from checkpoint, the shape in current model is torch.Size([4]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6130/3281781941.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m\"generator_ema\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"generator_ema\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/data/mrjaehong/handwriting_gen/fewshot-font-generation/env/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1496\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1497\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m-> 1498\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m   1499\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Generator:\n\tsize mismatch for style_emb_blocks.last.w: copying a param with shape torch.Size([8, 256, 1, 1]) from checkpoint, the shape in current model is torch.Size([4, 256, 1, 1]).\n\tsize mismatch for style_emb_blocks.last.b: copying a param with shape torch.Size([8]) from checkpoint, the shape in current model is torch.Size([4]).\n\tsize mismatch for style_emb_blocks.skip.w: copying a param with shape torch.Size([8, 128, 1, 1]) from checkpoint, the shape in current model is torch.Size([4, 128, 1, 1]).\n\tsize mismatch for style_emb_blocks.skip.b: copying a param with shape torch.Size([8]) from checkpoint, the shape in current model is torch.Size([4]).\n\tsize mismatch for comp_emb_blocks.last.w: copying a param with shape torch.Size([8, 256, 1, 1]) from checkpoint, the shape in current model is torch.Size([4, 256, 1, 1]).\n\tsize mismatch for comp_emb_blocks.last.b: copying a param with shape torch.Size([8]) from checkpoint, the shape in current model is torch.Size([4]).\n\tsize mismatch for comp_emb_blocks.skip.w: copying a param with shape torch.Size([8, 128, 1, 1]) from checkpoint, the shape in current model is torch.Size([4, 128, 1, 1]).\n\tsize mismatch for comp_emb_blocks.skip.b: copying a param with shape torch.Size([8]) from checkpoint, the shape in current model is torch.Size([4])."
     ]
    }
   ],
   "source": [
    "###############################################################\n",
    "weight_path = \"temp/outputs2/checkpoints/last.pth\"  # path to weight to infer\n",
    "emb_dim = 8\n",
    "decomposition = \"data/kor/decomposition.json\"\n",
    "primals = \"data/kor/primals.json\"\n",
    "###############################################################\n",
    "\n",
    "decomposition = json.load(open(decomposition))\n",
    "primals = json.load(open(primals))\n",
    "n_comps = len(primals)\n",
    "\n",
    "def decompose_to_ids(char):\n",
    "    dec = decomposition[char]\n",
    "    comp_ids = [primals.index(d) for d in dec]\n",
    "    return comp_ids\n",
    "\n",
    "cfg = Config(\"cfgs/LF/p2/default.yaml\")\n",
    "\n",
    "gen = Generator(n_comps=n_comps, emb_dim=emb_dim).cuda().eval()\n",
    "weight = torch.load(weight_path)\n",
    "if \"generator_ema\" in weight:\n",
    "    weight = weight[\"generator_ema\"]\n",
    "gen.load_state_dict(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['comp_enc.layers.0.conv.weight', 'comp_enc.layers.0.conv.bias', 'comp_enc.layers.1.conv.weight', 'comp_enc.layers.1.conv.bias', 'comp_enc.layers.2.gc.k_proj.weight', 'comp_enc.layers.2.gc.k_proj.bias', 'comp_enc.layers.2.gc.transform.0.weight', 'comp_enc.layers.2.gc.transform.0.bias', 'comp_enc.layers.2.gc.transform.1.weight', 'comp_enc.layers.2.gc.transform.1.bias', 'comp_enc.layers.2.gc.transform.3.weight', 'comp_enc.layers.2.gc.transform.3.bias', 'comp_enc.layers.3.conv.weight', 'comp_enc.layers.3.conv.bias', 'comp_enc.layers.4.ChannelGate.mlp.1.weight', 'comp_enc.layers.4.ChannelGate.mlp.1.bias', 'comp_enc.layers.4.ChannelGate.mlp.3.weight', 'comp_enc.layers.4.ChannelGate.mlp.3.bias', 'comp_enc.layers.4.SpatialGate.spatial.conv.weight', 'comp_enc.layers.4.SpatialGate.spatial.bn.weight', 'comp_enc.layers.4.SpatialGate.spatial.bn.bias', 'comp_enc.layers.4.SpatialGate.spatial.bn.running_mean', 'comp_enc.layers.4.SpatialGate.spatial.bn.running_var', 'comp_enc.layers.4.SpatialGate.spatial.bn.num_batches_tracked', 'comp_enc.layers.5.bias', 'comp_enc.layers.6.conv1.conv.weight', 'comp_enc.layers.6.conv1.conv.bias', 'comp_enc.layers.6.conv2.conv.weight', 'comp_enc.layers.6.conv2.conv.bias', 'comp_enc.layers.7.ChannelGate.mlp.1.weight', 'comp_enc.layers.7.ChannelGate.mlp.1.bias', 'comp_enc.layers.7.ChannelGate.mlp.3.weight', 'comp_enc.layers.7.ChannelGate.mlp.3.bias', 'comp_enc.layers.7.SpatialGate.spatial.conv.weight', 'comp_enc.layers.7.SpatialGate.spatial.bn.weight', 'comp_enc.layers.7.SpatialGate.spatial.bn.bias', 'comp_enc.layers.7.SpatialGate.spatial.bn.running_mean', 'comp_enc.layers.7.SpatialGate.spatial.bn.running_var', 'comp_enc.layers.7.SpatialGate.spatial.bn.num_batches_tracked', 'comp_enc.layers.8.conv1.conv.weight', 'comp_enc.layers.8.conv1.conv.bias', 'comp_enc.layers.8.conv2.conv.weight', 'comp_enc.layers.8.conv2.conv.bias', 'comp_enc.layers.9.conv1.conv.weight', 'comp_enc.layers.9.conv1.conv.bias', 'comp_enc.layers.9.conv2.conv.weight', 'comp_enc.layers.9.conv2.conv.bias', 'comp_enc.layers.9.skip.weight', 'comp_enc.layers.9.skip.bias', 'comp_enc.layers.10.ChannelGate.mlp.1.weight', 'comp_enc.layers.10.ChannelGate.mlp.1.bias', 'comp_enc.layers.10.ChannelGate.mlp.3.weight', 'comp_enc.layers.10.ChannelGate.mlp.3.bias', 'comp_enc.layers.10.SpatialGate.spatial.conv.weight', 'comp_enc.layers.10.SpatialGate.spatial.bn.weight', 'comp_enc.layers.10.SpatialGate.spatial.bn.bias', 'comp_enc.layers.10.SpatialGate.spatial.bn.running_mean', 'comp_enc.layers.10.SpatialGate.spatial.bn.running_var', 'comp_enc.layers.10.SpatialGate.spatial.bn.num_batches_tracked', 'comp_enc.layers.11.conv1.conv.weight', 'comp_enc.layers.11.conv1.conv.bias', 'comp_enc.layers.11.conv2.conv.weight', 'comp_enc.layers.11.conv2.conv.bias', 'style_emb_blocks.last.w', 'style_emb_blocks.last.b', 'style_emb_blocks.skip.w', 'style_emb_blocks.skip.b', 'comp_emb_blocks.last.w', 'comp_emb_blocks.last.b', 'comp_emb_blocks.skip.w', 'comp_emb_blocks.skip.b', 'content_enc.net.0.conv.weight', 'content_enc.net.0.conv.bias', 'content_enc.net.1.conv.weight', 'content_enc.net.1.conv.bias', 'content_enc.net.2.conv.weight', 'content_enc.net.2.conv.bias', 'content_enc.net.3.conv.weight', 'content_enc.net.3.conv.bias', 'content_enc.net.4.conv.weight', 'content_enc.net.4.conv.bias', 'decoder.layers.0.integrate_layer.conv.weight', 'decoder.layers.0.integrate_layer.conv.bias', 'decoder.layers.1.conv1.conv.weight', 'decoder.layers.1.conv1.conv.bias', 'decoder.layers.1.conv2.conv.weight', 'decoder.layers.1.conv2.conv.bias', 'decoder.layers.2.conv1.conv.weight', 'decoder.layers.2.conv1.conv.bias', 'decoder.layers.2.conv2.conv.weight', 'decoder.layers.2.conv2.conv.bias', 'decoder.layers.3.conv1.conv.weight', 'decoder.layers.3.conv1.conv.bias', 'decoder.layers.3.conv2.conv.weight', 'decoder.layers.3.conv2.conv.bias', 'decoder.layers.4.conv.weight', 'decoder.layers.4.conv.bias', 'decoder.layers.5.conv.weight', 'decoder.layers.5.conv.bias', 'decoder.layers.6.conv.weight', 'decoder.layers.6.conv.bias', 'decoder.layers.7.conv.weight', 'decoder.layers.7.conv.bias', 'decoder.skip_layer.integrate_layer.conv.weight', 'decoder.skip_layer.integrate_layer.conv.bias'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Load reference images.\n",
    "* `ref_path`: \n",
    "    * The path of reference font or images.\n",
    "    * If you are using a ttf file, set this to the location of the ttf file.\n",
    "    * If you want to use rendered images, set this to the path to the directory which contains the reference images.\n",
    "* `ref_chars`:\n",
    "    * The characters of reference images.\n",
    "    * If this is `None`, all the available images will be loaded.\n",
    "* `extension`:\n",
    "    * If you are using ttf files, set this to \"ttf\".\n",
    "    * If you are using image files, set this to their extension(png, jpg, etc..)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################\n",
    "ref_path = \"data_example/kor/png\"\n",
    "extension = \"png\"\n",
    "ref_chars = \"값넋닻볘츄퀭핥훟\"\n",
    "## Comment upper lines and uncomment lower lines to test with ttf files.\n",
    "# extension = \"ttf\"\n",
    "# ref_chars = \"값같곬곶깎넋늪닫닭닻됩뗌략몃밟볘뺐뽈솩쐐앉않얘얾엌옳읊죡쮜춰츄퀭틔핀핥훟\"\n",
    "########################################################\n",
    "\n",
    "ref_dict, load_img = load_reference(ref_path, extension, ref_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Generate the images.\n",
    "* `gen_chars`: The characters to generate.\n",
    "* `save_dir`: Path to save the generated images.\n",
    "* `source_path`: Path to the source.\n",
    "* `source_ext`: Extension of the source file. If you are using image files, set this to the image's extension.\n",
    "* `batch_size`: The number of images inferred at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################\n",
    "gen_chars = \"좋은하루되세요\"  # Characters to generate\n",
    "save_dir = \"./result/lf\"  # Directory where you want to save generated images\n",
    "source_path = \"data/kor/source.ttf\"\n",
    "source_ext = \"ttf\"\n",
    "batch_size = 16\n",
    "########################################################\n",
    "\n",
    "infer_LF(gen, save_dir, source_path, source_ext, gen_chars, ref_dict, load_img,\n",
    "         decomposition, primals, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
